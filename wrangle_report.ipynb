{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Wrangleing Report\n",
    "\n",
    "This report summarizes data wrangling efforts done to enable the analysis of WeRateDogs Twitter data. It involved data collection from multiple sources, assessment of quality and tidiness, and cleaning for analysis.\n",
    "\n",
    "### Data Gathering\n",
    "Three datasets were retrieved:\n",
    "\n",
    "Twitter archive: This was provided as a CSV file.\n",
    "\n",
    "Image Predictions: Downloaded from URL using the requests library\n",
    "\n",
    "Extra tweet data (favorites count,retweet count): Gathered using the Twitter API.\n",
    "\n",
    "Certainly, I'll rewrite this in a more comprehensive way with additional details in Markdown format:\n",
    "\n",
    "# Data Quality and Tidiness Assessment\n",
    "\n",
    "## Quality Issues\n",
    "\n",
    "### 1. Accuracy\n",
    "\n",
    "#### Twitter Archive Table\n",
    "- **Name column**: Contains incorrect or placeholder names (e.g., \"a\").\n",
    "- **Rating columns**: \n",
    "  - `rating_numerator` and `rating_denominator` contain unrealistic values, including zeros.\n",
    "\n",
    "\n",
    "### 2. Completeness\n",
    "\n",
    "#### Twitter Archive Table\n",
    "- Nine columns have more than 2,000 null values each.\n",
    "  - Specific columns should be identified.\n",
    "  - The total number of rows in the dataset should be provided for context.\n",
    "  - The percentage of missing data for each affected column should be calculated.\n",
    "\n",
    "#### Extra Archive Table\n",
    "- 13 columns have over 90% null values.\n",
    "- Three columns are entirely composed of NaN values (100% missing).\n",
    "  - These columns should be listed explicitly.\n",
    "\n",
    "\n",
    "### 3. Consistency\n",
    "\n",
    "#### Twitter Archive Table\n",
    "- The `expanded_urls` column contains duplicate values.\n",
    "  - The frequency and nature of these duplicates should be analyzed.\n",
    "\n",
    "\n",
    "#### Image Predictions Table\n",
    "- `jpg_url` column inconsistencies:\n",
    "  - Two images are PNG files instead of JPG.\n",
    "  - Some URLs are duplicated, potentially causing confusion.\n",
    "\n",
    "\n",
    "### 4. Validity\n",
    "\n",
    "#### Twitter Archive Table\n",
    "- `timestamp` column is not in the correct datetime format.\n",
    "\n",
    "\n",
    "#### Extra Archive Table\n",
    "- `id_str` is stored as an integer instead of a string.\n",
    "\n",
    "\n",
    "## Tidiness Issues\n",
    "\n",
    "### 1. Twitter Archive Table\n",
    "- Dog \"stage\" columns (`doggo`, `floofer`, `pupper`, `puppo`) should be consolidated into a single column.\n",
    "\n",
    "\n",
    "### 2. Extra Archive Table\n",
    "- Column names need to be renamed to better indicate the nature of their values.\n",
    "\n",
    "Certainly! I'll rewrite the cleaning steps as if you're speaking, outlining your plan to address the issues:\n",
    "\n",
    "#  Data Cleaning \n",
    "\n",
    "\n",
    "## 1. Twitter Archive Table\n",
    "\n",
    "### Accuracy Issues\n",
    "1. I'm going to start by fixing that `name` column. I'll scan through it and replace any obviously incorrect names like \"a\" with NaN. If there's a pattern to these errors, I might be able to automate this.\n",
    "\n",
    "2. For the `rating_numerator` and `rating_denominator` columns, I'll:\n",
    "   - Remove any ratings with zeros\n",
    "   - Cap the ratings at a reasonable maximum (probably 15/10, since we're dealing with good dogs here)\n",
    "   - If there are any negative ratings, I'll investigate those individually\n",
    "\n",
    "### Completeness Issues\n",
    "3. I've got 9 columns with over 2000 null values. I need to:\n",
    "   - List out these columns\n",
    "   - Calculate the percentage of nulls for each\n",
    "   - Decide whether to drop columns that are mostly empty or if I can fill in some of the missing data\n",
    "\n",
    "### Consistency Issues\n",
    "4. For the `expanded_urls` column:\n",
    "   - I'll check for exact duplicates and remove them\n",
    "   - If there are near-duplicates (like the same URL with different parameters), I'll standardize them\n",
    "\n",
    "### Validity Issues\n",
    "5. I'm going to convert that `timestamp` column to a proper datetime format. I'll use pandas for this - something like `pd.to_datetime()` should do the trick.\n",
    "\n",
    "### Tidiness Issues\n",
    "6. Those dog \"stage\" columns (`doggo`, `floofer`, `pupper`, `puppo`) need to be combined:\n",
    "   - I'll create a new `dog_stage` column\n",
    "   - Then I'll fill it with the value from whichever of the four columns isn't null\n",
    "   - If multiple stages are present, I'll concatenate them with a separator\n",
    "   - Finally, I'll drop the original four columns\n",
    "\n",
    "## 2. Extra Archive Table\n",
    "\n",
    "### Completeness Issues\n",
    "7. I've got 13 columns that are more than 90% nulls, and 3 that are completely empty:\n",
    "   - I'll list out all these problematic columns\n",
    "   - For the completely empty ones, I'm just going to drop them\n",
    "   - For the others, I'll decide case-by-case if they're worth keeping\n",
    "\n",
    "### Validity Issues\n",
    "8. The `id_str` column needs to be a string, not an int:\n",
    "   - I'll convert it using `df['id_str'] = df['id_str'].astype(str)`\n",
    "   - Then I'll check to make sure no data was lost in the conversion\n",
    "\n",
    "### Tidiness Issues\n",
    "9. Time to rename some columns:\n",
    "   - I'll make a list of all the ambiguous column names\n",
    "   - Then I'll come up with more descriptive names for each\n",
    "   - Finally, I'll use `df.rename(columns={...})` to apply the new names\n",
    "\n",
    "## 3. Image Predictions Table\n",
    "\n",
    "### Consistency Issues\n",
    "10. For the `jpg_url` column:\n",
    "    - I'll identify those two PNG files and decide whether to keep them or not\n",
    "    - I'll find and list out all the duplicate URLs\n",
    "    - Depending on how many dupes there are, I might keep them and add a flag, or I might remove them\n",
    "\n",
    "## 4. Final Steps\n",
    "\n",
    "### Storing\n",
    "The 3 cleaned data sets were combined into one master dataset which was stored as 'twitter_archive_master.csv'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
